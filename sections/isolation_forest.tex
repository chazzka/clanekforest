\section{Isolation Forest}
\label{sec:isolation_forest}

Isolation Forest
\cite{liu2008isolation, liu2012isolation} is an outlier
detection, unsupervised ensemble algorithm. This approach is well-known for successfully identifying outliers using recursive partitioning (forming a decision tree) to decide whether the analyzed data point is an anomaly. The fewer partitions required to isolate, the more probable it is for a point to be an anomaly.

The initial problem with using the original Isolation Forest for novelty detection is that a potential novelty point located far from training data tends to fall into some existing branch because the Isolation Forest's splits were created without seeing it, making its placement arbitrary and leading to incorrect isolation.
This statement will later be supported by mathematical analysis and revisited in \ref{sec:revisiting}.

In the following sections, we describe the original decision tree.
We deviated slightly from Liu et al.~\cite{liu2008isolation} for better interpretability and comparability with the proposed solution.
Note that this deviation is theoretical and does not affect the original functionality.

% \paragraph{Isolation tree}
% Isolation Tree is a rooted tree constructed with a subset of \(A\) items
% (datapoints) with the size \(s=|A|\).

% \begin{enumerate}
%     \item To build an isolation tree, it is not necessary to have a large set; it
%   may even be undesirable
%   \item Well-chosen small \(s\) can help eliminate \emph{masking} and
%   \emph{swamping}


% \begin{itemize}
%     \item \textbf{Masking} When the number of anomalies is high, it is possible that some of those aggregate in a dense and large cluster, making it more difficult to separate the single anomalies and, in turn, to detect such points as
%   anomalous.
%     \item \textbf{Swamping} When normal instances are too close to anomalies, the number of
%   partitions required to separate them increases, making it more difficult for the Isolation Forest to discriminate between
%   anomalies and normal points.
% \end{itemize}

% \item There are two types of vertices

% \begin{itemize}
%     \item \textbf{Internal vertex} Internal vertex contains a condition (a feature and a limit) and two children (one
%   representing a fulfilled condition and the other an unfulfilled one)
%     \item \textbf{External vertex} External vertex is created if the conditions of the parents are met (or not met) by
%   one or none of the elements from the sample or the maximum depth of
%   the tree \(l\) is reached, usually \(l=\ln_2(s)\). It contains the
%   evaluation of \(h(x)\) using the distance from the root. If the max
%   length of the tree is reached the \emph{distance} is estimated using
%   \(h(x)=e+c(n)\), where \(e\) is the distance from the root, \(n\) is
%   the number of elements from the sample satisfying the conditions of
%   the parents, \(c(n)=2\,(H_{n-1}-\frac{n-1}{n})\) and \(H_{n-1}\) is
%   \(n-1\) harmonic number.
% \end{itemize}

% \end{enumerate}

\subsection{Definitions}
\begin{definition}
Let $\mathsf{s}=(s_0, \dots, s_d, \dots, s_{n-1})\in \mathbb{R}^n$ be a data point. Then we say that $\pi_d(\mathsf{s}):=s_d$ is a \emph{projection} of data point $\mathsf{s}$ onto  dimension $d$  yielding $s_d$.
\end{definition}

\begin{definition}
Let $Z$ be a finite subset of $\mathbb{R}^n$,
$$Z \subseteq \mathbb{R}^n ;\quad n \in \mathbb{N}.$$

For each dimension \(d \in\{0, \dots, n - 1\}\), let
$$Z_d = \{ \pi_d(\mathsf{s})\ |\ \mathsf{s} \in Z \}.$$

Then we define a hyperrectangle $R(Z)$ \emph{surrounding} $Z$, such that
$$R(Z) = r_0 \times r_1 \times \cdots \times r_{n-1},$$ where $r_d = \langle \min Z_d, \max Z_d \rangle$ for each $d \in \{0,1, \dots, n-1\}.$

\end{definition}


\subsection{Constructing a decision tree}
\label{sec:cdt}
\begin{enumerate}

    \item Each leaf or internal vertex is a hyperrectangle $R$. 
    \item Function relation $\rho$ assigns the split point $z$ and the dimension $d$ to the internal vertex $v$,
    $$(v, (z,d)) \in \rho.$$
    % \item The maximum possible height of a tree is controlled by the \emph{max\_depth} parameter.
    \item The sample \(S\) contains \emph{b} number of input points of $n$ dimensions, that is
    $$S \subseteq \mathbb{R}^n ;\quad |S| = b; \quad b \in \mathbb{N}.$$
    \item The ending condition of the leaves is satisfied if the vertex \(R\) satisfies \[| S \cap R | = 1.\]
%     \item Leaves' ending condition is satisfied by one of two criteria: 
% \begin{enumerate}
%     \item Vertex \(R\) satisfies \(| S \cap R | = 1\).
%     \item Vertex's depth reached the \emph{max\_depth} value.
% \end{enumerate}
\end{enumerate}


\paragraph{Basis step}
The trivial rooted tree \(T_0\) is a tuple with
vertices \(V_0 = \{R(S)\}\) and edges \(E_0 = \emptyset\), i.e. 
\[T_0= (V_0, E_0) = (\{R(S)\},\emptyset).\]
The function relation $\rho_0$ initially has no assignments, i.e.
$$\rho_0 = \emptyset.$$

% The steps to reach the tree \(T_{j+1}\) from \(T_{j} = (V_j, E_j)\) and $\rho_{j+1}$ from $\rho_{j}$ are
% as follows:
% Let us create \(T_{j+1}\) from \(T_{j} = (V_j, E_j)\) and $\rho_{j+1}$ from $\rho_{j}$.
% The creation of \(T_{j+1}\) from \(T_{j} = (V_j, E_j)\) and $\rho_{j+1}$ from $\rho_{j}$ starts by forming a new hyperrcentagle R.
% to reach the tree \(T_{j+1}\) from \(T_{j} = (V_j, E_j)\) and $\rho_{j+1}$ from $\rho_{j}$
\paragraph{Recursive step}
The task of this step is to reach the tree \(T_{j+1}\) from \(T_{j} = (V_j, E_j)\) and $\rho_{j+1}$ from $\rho_{j}.$
Let \(L_j \subseteq V_j\) be a subset of leaves not satisfying the
ending condition
% By selecting a random dimension $d_R$ (such that $|r_{d_R}| > 1$) create two new vertices \(R_l, R_r\) for each leaf \(R \in L_j\)\
% \[R =  r_0 \times \cdots \times r_{d_R} \times \cdots \times r_{n-1} \] with a random split point $z_R \in r_{d_R}$.
and \(R \in L_j\)\ be a leaf such that
\[R =  r_0 \times \cdots \times r_{d_R} \times \cdots \times r_{n-1}. \]
We first select a random dimension $d_R$ (such that $r_{d_R}$ is infinite) and a random split point $z_R \in r_{d_R}$.
The split point $z_R$ splits $R \cap S$, creating two disjunctive sets $S_l$ and $S_r$ respectively, such that
\begin{align*}
S_l &= \{ \mathsf{s} \in{R \cap S}\ |\ \pi_{d_R}(\mathsf{s})\le z_R\},\\
S_r &= \{ \mathsf{s} \in{R \cap S}\ |\ \pi_{d_R}(\mathsf{s}) > z_R\}.
\end{align*}
Then we obtain left and right hyperrectangles \(R_l\), \(R_r\) as
follows:
\begin{align*}
R_l &= R(S_l),&
R_r &= R(S_r).
\end{align*}

Each vertex \(R \in L_j\) is associated with two new
edges \((R,R_l ), (R, R_r)\) and is assigned with $(z_R,d_R)$ by function relation $\rho_{j+1}$, such that


\begin{align*}
   \rho_{j+1} &= \rho_j \cup \bigcup_{R \in L_j} \{(R, (z_R, d_R))\}, \\
   V_{j+1} &= V_j \cup \bigcup_{R \in L_j} \{R_l, R_r\}, \\
   E_{j+1} &= E_j \cup \bigcup_{R \in L_j} \{(R, R_l), (R,R_r)\},\\
   T_{j+1} &= (V_{j+1}, E_{j+1}),
\end{align*}
i.e.~${R_l, R_r} \subset R$ are leaves and $R$ is an inner vertex in the new tree
\(T_{j+1}\).\footnote{Tree \(T_{j+1}\) is actually a Hasse diagram of the ordered set
\((V_{j+1},\subseteq)\)}


\paragraph{Termination} The algorithm moves to the next recursion step unless there is an equality of two consecutive trees \(T_k = T_{k+1}\). Such equality happens when all leaves satisfy their ending condition, i.e., \(L_k = \emptyset\).
Thus, the desired tree $T$ is the tree $T_k$, and the finite relation $\rho$ is $\rho_k$.


\subsection{Example of decision tree construction}
\label{example:original_tree_create}
Consider now an example of creating a new Isolation tree based on the given input sample
\begin{align*}
    S = \left\{\begin{smallmatrix}
    [25,100],&[30,90],&[20,90],&[35,85],\\
    [25,85],&[15,85],&[105,20],&[95,25], \\
    [95,15],&[90,30],&[90,20],&[90,10]
    \end{smallmatrix}\right\},
\end{align*}
as shown in Figure \ref{fig:example_noutlier_gnu}.

\begin{figure*}[!t]
\centering
\includesvg[width=0.9\textwidth, inkscapelatex=false]{figures/example6_noutlier_gnu_legend.svg}
\caption{Original solution. Rectangles created by recursive splitting.}
\label{fig:example_noutlier_gnu}
\end{figure*}



The tree $T$ is created by starting with the tree $T_0$ and expanding further as described by the recursive step until the ending condition is met.
Figure \ref{fig:example_noutlier_tree_color} shows the finished tree $T$, trained on the dataset $S$. Numbers represent the final leavesâ€™ depth.

\paragraph{Basis step} 
We create the tree $T_0$ and a function relation $\rho_0$.
   There is just a single vertex (root) 
   \[R = R(S) = \langle 15, 105 \rangle \times \langle 10, 100 \rangle,\]
   without any connections, so both $E_0$ and $\rho_0$ are empty, i.e.
\begin{align*}
V_0 &= \{R\},& E_0 &= \emptyset,\\
T_0 &= (V_0, E_0),& \rho_0 &= \emptyset.
\end{align*}
Figure \ref{fig:example_noutlier_gnu} shows $R$ as the largest and the brightest coloured rectangle.

\paragraph{Recursive step} 
In order to reach $T_1$ from $T_0$, a dimension $d_R=1$ and a split point $z_R = 72.63$ were randomly chosen. Figure \ref{fig:example_noutlier_gnu} shows split points as purple lines in their respective rectangles. The recursive step starts with creating left and right descendants of $R$ as
\begin{align*}
S_l &= \left\{\begin{smallmatrix}
 [105,20],& [95,25], &[95,15],\\
[90,30],&[90,20],&[90,10]
\end{smallmatrix}\right\}, \\
S_r &= \left\{\begin{smallmatrix}
    [25,100],&[30,90],&[20,90],\\
    [35,85], &[25,85],&[15,85]
\end{smallmatrix}\right\}.
\end{align*}
forming a new tree $T_1$ and a new function relation $\rho_1$ as follows:
\begin{align*}
V_1 &= \{R, R_l, R_r\},&
E_1 &= \{(R,R_l), (R,R_r)\},\\
T_1 &= (V_1, E_1),&
\rho_1 &= \{ (R, (72.63, 1))\}.
\end{align*}
The tree $T_1$ has the left leaf $R_l$ and the right leaf $R_r$; the ending condition is not met, that is $L_1 = \{R_l,R_r\}$. 

We continue the recursive step with two vertices.
For the left vertex $R_l$, the dimension $d_{R_l} =0$ and the split point $z_{R_L}= 103.08$ were chosen randomly, giving
\begin{align*}
S_{ll} &= \left\{\begin{smallmatrix}
    [95,25],& [95,15],\\ 
    [90,30],&[90,20],\\
    [90,10]
\end{smallmatrix}\right\},&
S_{lr} &= \{[105,20]\},\\
R_{ll} &= \langle 90, 95 \rangle \times \langle 10, 30\rangle,&
R_{lr} &= \langle 105, 105 \rangle \times \langle 20, 20\rangle
\end{align*}
and $z_{R_r}= 20.32$, $d_{R_r} = 0$ for the right vertex $R_r$ respectively
\begin{align*}
S_{rl}&= \{[20,90],[15,85]\},&
S_{rr} &= \left\{\begin{smallmatrix}
    [25, 100],& [30,90],\\
    [35,85],& [25,85],
\end{smallmatrix}\right\},\\
%S_{rr}&= \{[25,100],[30,90],[35,85],[25,85]\}\\
R_{rl}&= \langle 15, 20 \rangle \times \langle 85, 90 \rangle,&
R_{rr}&= \langle 25, 35 \rangle \times \langle 85, 100 \rangle.
\end{align*}

With the rectangles prepared, we can assemble new vertices and edges and create a new $T_2$:
\begin{align*}
\rho_2 &= \{(R,(72.63,1), (R_l, (103.08, 0)), (R_r, (20.32, 0)) \},\\
V_2 &= \{ R, R_l, R_r, R_{lr}, R_{lr}, R_{rl}, R_{rr} \},\\
E_2 &= \left\{\begin{smallmatrix} 
(R,R_l),&(R,R_r), &(R_l,R_{ll}),\\ 
(R_l,R_{lr}),& (R_r,R_{rl}),& (R_r,R_{rr})
\end{smallmatrix}\right\},\\
T_2 &= (V_2, E_2).
\end{align*}

\paragraph{Termination} With the $T_2$ created, we now have to check for the ending condition of the leaves. Since $|R_{lr}\cap S|=|S_{lr}| = 1$, the ending condition for the leaf $R_{lr}$ is met, and the new set of leaves $L_2$ for the next recursion step is
$$L_2 = \{R_{ll},R_{rl},R_{rr}\}.$$

This continues until we reach tree $T_5$ such that $T_5=T_6$ is the desired tree $T$ as shown in Figure \ref{fig:example_noutlier_tree_color}. 



\subsection{Evaluating decision tree}
The evaluation of desired element $a$ starts in the root $R$ of the previously built tree $T$.
In a root, by applying function relation, $\rho(R) = (z,d)$, we obtain split point $z$ and dimension $d$.
The root $R$ of a tree $T$ has two descendants $R_l$, $R_r$, such that
$\forall r_l\in R_l; \pi_d(r_l) \le z$ and $\forall r_r\in R_r; \pi_d(r_r)  > z$.

If $\pi_d(a)\le z$, we visit $R_l$, else, that is $\pi_d(a) > z$, we visit $R_r$.
We continue in this manner until we reach the leaf. Figures \ref{fig:example_noutlier_gnu} and \ref{fig:example_noutlier_tree_color} show the final depths of individual leaves. Based on them, we can decide on the level of outlierness. The deeper the evaluated point in the tree, the less anomalous it gets.

\begin{example}
\label{ex:regular_point_evaluation_original}
    Consider evaluating the point $a = [105,20]$ on the tree $T$ built in Example \ref{example:original_tree_create}.

    We start with the root $R = \langle 15,105\rangle \times \langle 10, 100 \rangle$.
    By applying the function $\rho(R)$, we obtain the split point $z = 72.63$ and the dimension $d = 1$.
    The root $R$ has two descendants 
\begin{align*}
    &R_l = \langle 90,105\rangle \times \langle 10, 30 \rangle,&
    &R_r = \langle 15,35\rangle \times \langle 85, 100 \rangle,\\
    \intertext{such that}
    &\forall r_l\in R_l; \pi_1(r_l) \le 72.63,&
    &\forall r_r\in R_r; \pi_1(r_r) > 72.63.
\end{align*}
Now, by applying the projection $\pi_1$ on $a$, we obtain $20$, which is less than the split point $z = 72.63$, i.e.
$$\pi_1([105,20]) = 20 < 72.63.$$
We visit the vertex $R_l$ because the value obtained by applying the projection on any element of $R_l$ is smaller than $72.63$.

We reached the next recursive step. With the vertex $R_l$ visited, we apply the function $\rho(R_l)$, obtaining the new split point $z = 103.08$ and the new dimension $d = 0$.
The vertex $R_l$ has two descendants 
\begin{align*}
    &R_{ll} = \langle 90,95\rangle \times \langle 10, 30 \rangle,&
    &R_{lr} = \langle 105,105\rangle \times \langle 20,20 \rangle,\\
    \intertext{such that}
    &\forall r_{ll}\in R_{ll}; \pi_0(r_{ll}) \le 103.08,&
    &\forall r_{lr}\in R_{lr}; \pi_0(r_{lr}) > 103.08.
\end{align*}

We visit the vertex $R_{lr}$ because the value obtained by applying the projection $\pi_0$ on any element of $R_{lr}$ and also on $a$ is more than $103.08$.

Since $R_{lr}$ has no descendants, we reached the final leaf with the depth of~$2$.
\end{example}


\begin{example}
\label{ex:novelty_point_evaluation_original}
    Consider now the evaluation of $a' = [25,20]$ on the tree $T$ built in Example \ref{example:original_tree_create}. Note that $a'$ was not contained in the training set for building a tree.

    We start with the root $R = \langle 5,105\rangle \times \langle 10, 100 \rangle$.
    By applying function $\rho(R)$, we obtain the split point $z = 72.63$ and the dimension $d = 1$.
%     Root $R$ has two descendants 
% \begin{align*}
%     &R_l = \langle 90,105\rangle \times \langle 10, 30 \rangle,&
%     &R_r = \langle 15,35\rangle \times \langle 85, 100 \rangle,\\
%     \intertext{such that}
%     &\forall r_l\in R_l; \pi_1(r_l) \le 72.63,&
%     &\forall r_r\in R_r; \pi_1(r_r) > 72.63.
% \end{align*}
% Now, by applying projection $\pi_1$ on $a'$, we obtain $20$, which is less than split point $z = 72.63$, i.e.
We visit $R_l = \langle 90,105\rangle \times \langle 10, 30 \rangle$ 
because $\pi_1([25,20]) = 20 \le 72.63.$
% We visit the vertex $R_l$ because the value obtained by applying the projection on any element of $R_l$ is less than $72.63$.

We reached the next recursive step. Obtaining the new split point $z = 103.08$ and the new dimension $d = 0$, we visit $R_{ll} = \langle 90,95\rangle \times \langle 10, 30 \rangle$, since $\pi_0([25,20]) \le 103.08$.
% Vertex $R_l$ has two descendants 
% \begin{align*}
%     &R_{ll} = \langle 90,95\rangle \times \langle 10, 30 \rangle,&
%     &R_{lr} = \langle 105,105\rangle \times \langle 20,20 \rangle,\\
%     \intertext{such that}
%     &\forall r_{ll}\in R_{ll}; \pi_0(r_{ll}) \le 103.08,&
%     &\forall r_{lr}\in R_{lr}; \pi_0(r_{lr}) > 103.08.
% \end{align*}

% We visit the vertex $R_{ll}$ because the value obtained by applying the projection on any element of $R_{ll}$ is less than $103.08$.

This repeats recursively until the leaf $R_{llllr}$ is reached. Note that this is the leaf with the point $[90,20]$. The reached depth is 5, as shown in Figure \ref{fig:example_noutlier_tree_color} (the leaf is marked grey).


Figure \ref{fig:example_noutlier_gnu} shows that after just two steps, the $[25,20]$ is no longer a part of any further evaluated rectangles.

% Figure \ref{fig:example_noutlier_gnu} shows that $[25,20]$ is evaluated in a way that it is left in the vertex with the point $[90,20]$, thus being evaluated as such.
\end{example}

Note that each element that was part of the batch during the training --- tree building --- phase is always contained in each vertex it visits. See $a \in R_{lr} \subset R_{l} \subset R$ in Example \ref{ex:regular_point_evaluation_original}.
This is not true for elements unseen during the training phase (such as novelty points).
See $a' \in R$, but $a' \notin R_l$ (and of course $a' \notin R_r$) in Example \ref{ex:novelty_point_evaluation_original}.

\subsection{Revisiting the Initial Problem}
\label{sec:revisiting}
Suppose we now want to use the original tree to evaluate a novelty data point $p$, which is not present in the training set.
Let us recall the equations from \textit{recursive step} in section \ref{sec:cdt} needed for vertex creation:
\begin{align*}
S_l &= \{ \mathsf{s} \in{R \cap S}\ |\ \pi_{d_R}(\mathsf{s})\le z_R\},\\
S_r &= \{ \mathsf{s} \in{R \cap S}\ |\ \pi_{d_R}(\mathsf{s}) > z_R\}.
\end{align*}
The point $p$ fits neither $S_l$ nor $S_r$ and does not even necessarily fit $R_l = R(S_l)$ nor $R_r = R(S_r)$.\footnote{For practical illustration, see point $a'$ in Example \ref{ex:novelty_point_evaluation_original}.}

Nevertheless, because the point $p$ satisfies  $\pi_d(p) \le z$ (or $\pi_d(p) > z$), $p$ \emph{is assigned} to the vertex $R_l$ (or $R_r$) even though $p \notin R_l$ nor $p \notin R_r$.

% This is why the original Isolation Forest is a purely unsupervised algorithm. To work correctly, the tree has to be constructed concerning all possible input data.


\begin{figure*}[!t]
\includesvg[inkscapelatex=false,width=1\textwidth]{figures/example6_Noutlier_tree_tb.svg}
\caption{tree constructed using the original approach}
\label{fig:example_noutlier_tree_color}
\end{figure*}


% \begin{sidewaysfigure}[htbp]
% \centering
% \includesvg[inkscapelatex=false,width=1\textwidth]{figures/example6_Noutlier_tree_tb.svg}
% \caption{tree constructed using the original approach}
% \label{fig:example_noutlier_tree_color}
% \end{sidewaysfigure}
