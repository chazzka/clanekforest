\section{Proposed Range-based Enhancement For Isolation Forest}
\label{sec:novelty_isolation_forest}
In this section, we propose a new enhancement of the original Isolation Forest algorithm to make it possible to detect novelty observations.
The proposed enhancement takes the basic idea of an ensemble of trees with various depths but takes it further to make semi-supervised novelty detection possible.

\subsection{Initial Problem}
 The standard Isolation Forest algorithm cannot be used for novelty detection. This is because in each step, it limits the observation with the previously separated data.

Suppose we now want to use the original tree to evaluate the novelty data point $p$, which is not present in the training set.

Let us recall the equations needed for vertex creation:
\begin{align*}
S_l &= \{ \mathsf{s} \in{R \cap S}\ |\ \pi_{d_R}(\mathsf{s})\le z_R\},&
S_r &= \{ \mathsf{s} \in{R \cap S}\ |\ \pi_{d_R}(\mathsf{s}) > z_R\}.
\end{align*}
The point $p$ fits neither $S_l$ nor $S_r$ and does not even necessarily fit $R_l = R(S_l)$ nor $R_r = R(S_r)$. 

Nevertheless, because datapoint $p$ satisfies  $\pi_d(p) \le z$ (or $\pi_d(p) > z$), $p$ is assigned to a vertex $R_l$ (or $R_r$) even though $p \notin R_l$ nor $p \notin R_r$.

This is why the original Isolation Forest is a purely unsupervised algorithm. To work properly, the tree has to be constructed concerning all possible input data.

\subsection{Proposed Solution}
The proposed solution comes from the idea that the original tree lacks the possibility to isolate more datapoints than it currently observes.
The observed space is bounded by the minimum and maximum in each feature.

As in the original article, we use the concept of a binary decision tree. The proposed solution is altering the concept of the split point evaluation. Whereas the original Isolation Forest evaluates the split point based on the previous data, we
evaluate the split point based on a range in our proposed solution. For this to work, several alterations to the split point evaluation and the form of data passed between vertices must be made; however, the overall concept of the forest remains the same.
The proposed solution has two main concepts of alteration to the original solution.

\begin{enumerate}
    \item Each of the vertices gets assigned a space bounded by ranges. Each range should be reasonable enough to separate all the domain space correctly.
    \item The split point is in the middle of the given dimensionâ€™s range.
    \item The input observations are only used to determine the ending condition.
\end{enumerate}

\subsection{Constructing decision tree $T$}

\begin{enumerate}
    \item The maximum possible depth of a tree is controlled by the \emph{max\_depth} parameter.
    \item The sample \(S\) contains \emph{batch\_size} number of input datapoints.
    \item Leaves and internal vertices are possibility-space hyperrectangles \(R\). 
    \item Leaves' ending condition is satisfied by one of two criteria: 
\begin{enumerate}
    \item Vertex \(R\) satisfies \(S \cap R = \emptyset\) or \(| S \cap R | = 1\).
    \item Vertex's depth reached the \emph{max\_depth} value.
\end{enumerate}
\end{enumerate}



\subparagraph{Basis step}

Each dimension \(d \in\{0, \dots, n-1\}\), is bounded by the range \(r_d\). The ranges form the possibility-space hyperrectangle \(R_0\) as in:

\[R_0 =  r_0 \times r_1 \times \cdots \times r_{n-1}  \tag{xx}\,.\]

The trivial rooted tree \(T_0\) is a tuple with
vertices \(V_0 = \{R_0\}\) and edges \(E_0 = \emptyset\), i.e.
\[T_0= (V_0, E_0) = (\{R_0\},\emptyset).\]

\subparagraph{Recursive step}

The steps to reach the tree \(T_{j+1}\) from \(T_{j} = (V_j, E_j)\) are
as follows:

Let \(L_j \subseteq V_j\) be a subset of leaves not satisfying the
ending condition. 

For each leaf \(R \in L_j\) create two new vertices \(R_l, R_r\) by
selecting a random dimension \(d\).

Let
\begin{align}
R &= r_0 \times  \cdots \times r_{d-1} \times  r_d\times r_{d+1} \times \cdots \times r_{n-1},
\end{align}
where $r_d = \langle x, y ).$ %TODO: NEVIME k cemu to je

Then we obtain the left and right hyperrectangles \(R_l\), \(R_r\) as
follows:
\begin{align}
R_l &= r_1 \times  \cdots \times r_{d-1} \times  r_l \times r_{d+1} \times \cdots \times r_n \\
R_r &= r_1 \times  \cdots \times r_{d-1} \times  r_r \times r_{d+1} \times \cdots \times r_n
\end{align}
where \(r_l = \langle x, s )\) and \(r_r = \langle s, y )\) and \(s\) is
a number obtained as the middle of the range \(r_d\,\):

\[s = \frac{x + y}{2}\tag{x}.\] Each vertex \(R\) is associated with two new
edges \((R,R_l ), (R, R_r)\) as follows:
\begin{align*}
V_{j+1} &= V_j \cup \bigcup_{R \in L_j} \{R_l, R_r\},\\
E_{j+1} &= E_j \cup \bigcup_{R \in L_j} \{(R, R_l), (R,R_r)\},\\
T_{j+1} &= (V_{j+1}, E_{j+1}),
\end{align*}
i.e.~${R_l, R_r} \subset R$ are leaves in the new tree
\(T_{j+1}\).

Recursion is terminated if there is an equality of two consecutive trees \(T_j = T_{j+1}\). This happens when all leaves satisfy the ending condition, i.e., \(L_j = \emptyset\).
If this is the case, then the desired tree $T$ is the tree $T_j$ otherwise, move to the next recursion step.


Note that tree \(T_{j}\) is actually a Hasse diagram of the ordered set
\((V_j,\subseteq)\).


\begin{example}
\label{example:novelty_tree_create}
Consider now an example of creating a new enhanced Isolation tree based on the given input sample $S$.

\begin{align*}
    S = \{&[25,100],[30,90],[20,90],[35,85],\\
    &[25,85],[15,85],[105,20],[95,25], \\
    &[95,15],[90,30],[90,20],[90,10]\}
\end{align*}


\begin{figure}[htbp]
\centering
\includesvg[width=0.9\textwidth,inkscapelatex=false]{figures/example66_Novelty_gnu.svg}
\caption{example}
\label{fig:example_novelty_gnu}
\end{figure}

After selecting \emph{max depth} of 8 (experimentally), the tree $T$ is created by starting with tree $T_0$ and expanding further as described by the recursive step until the final condition is met.

Figure \ref{fig:example_novelty_gnu} shows the finished tree $T$, learned on dataset $S$.

\begin{enumerate}
    \item Basis step is to create a tree $T_0$ with one root vertex $R = \langle 0,110) \times \langle -5,105)$ with experimentally set initial possibility space and no edges $E_0$, such that
    \begin{align*}
        V_0 &= \{R\},&
        E_0 &= \emptyset,&
        T_0 &= (V_0, E_0)
    \end{align*}
    \item Since $R \in L_0$, we create two hyperrectangles $R_l$, $R_r$ by selecting a random dimension $d=0$.
    \begin{align*}
        r_0 &= \langle 0, 110) \\
        s &= \frac{0 + 110}{2} = 55 \\
        r_l &= \langle 0, 55) \\
        R_l &= \langle 0, 55) \times \langle -5,105) \\
        r_r &= \langle 55, 110) \\
        R_r &= \langle 55, 110) \times \langle -5,105)
    \end{align*}

    \item New tree $T_1$ is then
    \begin{align*}
    V_1 &= \{R, R_l, R_r\} \\
    E_1 &= \{(R, R_l), (R, R_r)\} \\
    T_1 &= (V_1, E_1)
    \end{align*}

    \item After checking for the ending condition, $L_1$ is as follows:
    $$L_1 = \{R_l, R_r\}$$

    \item Since there are vertices in $L_1$ left to be examined, we continue the next recursive step:
    Since $R_l \in L_1$ (resp. $R_r \in L_1$), we create two hyperrectangles $R_{ll}$, $R_{lr}$ --- left column (resp. $R_{rl}$, $R_{rr}$ -- right column) by selecting a random dimension $d=1$ (resp. $d=0$).
    \begin{align*}
        r_{l1} &= \langle -5, 105)& r_{r0} &= \langle 55, 110) \\
        s_l &= 50 & s_r&=82.5\\
        r_{ll} &= \langle -5, 50) & r_{rl} &= \langle 55, 82.5) \\
        R_{ll} &= \langle 0, 55) \times \langle -5,50) & R_{rl} &= \langle 55, 82.5) \times \langle -5,105)\\
        r_{lr} &= \langle 50, 105) & r_{rr} &= \langle 82.5, 110) \\
        R_{lr} &= \langle 0, 55) \times \langle 50,105) & R_{ll} &= \langle 82.5, 110) \times \langle -5,105)
    \end{align*}

    \item New tree $T_2$ is then
    \begin{align*}
        V_2 &= \{R, R_l, R_r, R_{ll}, R_{lr}, R_{rl}, R_{rr}\} \\
        E_2 &= \{(R, R_l), (R, R_r), (R_l, R_{ll}), (R_l, R_{lr}), (R_r, R_{rl}), (R_r, R_{rr})\} \\
        T_2 &= (V_2, E_2)
    \end{align*}

    \item After checking for the ending condition, $L_2$ is as follows:
    $$L_2 = \{R_{lr}, R_{rr}\}$$

    \item This goes on until one of the final conditions is met.

\end{enumerate}

\end{example}

\subsection{Evaluating enhanced decision tree $T$}
The evaluation of our enhanced decision tree is more straightforward since the examined datapoint is always contained in the possibility space hyperrectangle of some vertex in each depth until a leaf is visited.
The evaluation starts in the root vertex. The initial possibility space should be reasonable enough to cover the whole domain of a given problem.
Until the leaf is reached, the examined point recursively visits the ancestor within which it is contained.



\begin{example}
\label{ex:regular_point_evaluation_novelty}
    Consider now the evaluation of $a = [105,20]$ on tree $T$ built in Example \ref{example:novelty_tree_create}.

\begin{enumerate}
    \item  We start with the root $R = \langle 0,110\rangle \times \langle -5, 105 \rangle$.
    Root $R$ has two ancestors 
\begin{align*}
    &R_l = \langle 0,55) \times \langle -5, 105),&
    &R_r = \langle 55,110) \times \langle -5, 105),
\end{align*}
Since $a \in R_r$, we visit $R_r$.
\item Vertex $R_r$ has two ancestors
\begin{align*}
    &R_{rl} = \langle 5,82.5) \times \langle -5, 105),&
    &R_{rr} = \langle 82.5,110) \times \langle -5, 105),
\end{align*}
Since $a \in R_{rr}$, we visit vertex $R_{rr}$.
\item
we continue recursively in this manner for another two steps until leaf $R_{rrlr}$ is reached. This leaf has a depth of $4$.

\end{enumerate}
   
\end{example}

\begin{example}
\label{ex:novelty_point_evaluation_novelty}
    Consider now the evaluation of $a' = [25,20]$ on tree $T$ built in Example \ref{example:novelty_tree_create}.

\begin{enumerate}
    \item  We start with the root $R = \langle 0,110\rangle \times \langle -5, 105 \rangle$.
    Root $R$ has two ancestors 
\begin{align*}
    &R_l = \langle 0,55) \times \langle -5, 105),&
    &R_r = \langle 55,110) \times \langle -5, 105),
\end{align*}
Since $a' \in R_l$, we visit $R_l$.
\item Vertex $R_l$ has two ancestors
\begin{align*}
    &R_{ll} = \langle 0,55) \times \langle -5, 50),&
    &R_{lr} = \langle 0,5) \times \langle 50, 105),
\end{align*}
Since $a' \in R_{ll}$, we visit vertex $R_{ll}$.
\item
Since $R_{ll}$ is a leaf, we end here, and the reached leaf's depth is $2$.
\end{enumerate}
\end{example}
Note that each evaluated point of the given possibility space is always contained in each vertex it visits.
Hence $a \in R_{rrlr} \subset R_{rrl} \subset R_{rr} \subset R_{r} \subset R$ in \ref{ex:regular_point_evaluation_novelty}
and $a' \in R_{ll} \subset R_{l} \subset R$ as shown in Example \ref{ex:novelty_point_evaluation_novelty}.


Consider a minified example depicted in \ref{fig:example_data}. In this example, two chunks of data are in the top left and bottom right corners, respectively. Then, we selected a specific data point that shares one dimension similar to the first chunk and the other with the second one.

Note that the data have no specific distribution. The point at the bottom left has a value of $P_x = [5,5]$ and is not present for the learning phase.

\begin{figure}[htbp]
\centering
\includesvg[width=0.9\textwidth]{figures/example6_experiment.svg}
\caption{Example figure}
\label{fig:example_data}
\end{figure}

%TODO: ODTADYKA TO SMAZEME

\paragraph{Solution: Original approach}
With the original approach being fully unsupervised, we feed the whole input (excluding the $P_x$) to the forest and examine the resulting tree.

First, the trivial binary tree $T_0$ is created (xx), with ranges being the min-max values of input.
By doing the recursive steps, the whole tree is constructed.
Figure \ref{fig:example_noutlier_tree_color} shows the constructed binary tree based on the input data.

The evaluation of $P_x$ is as follows:
\begin{itemize}
    \item In the first step, after randomly selecting the dimension $d=1$ and split point $z$ as in (x) $z = 64.37$, the $P_x$ visits the node $\langle 102.0, 105.0\rangle \times \langle 3.0, 7.0\rangle$
    Note that it is enough for the point to fit only the selected dimension.
    \item In the second step, the point visits the node $\langle 102.0, 105.0\rangle \times \langle 5.0, 7.0\rangle$.
    \item This continues until the final leaf $\langle 102.0, 105.0\rangle \times \langle 5.0, 5.0\rangle$ marked gray in Figure \ref{fig:example_noutlier_tree_color}) is reached. This leaf has a depth of 4.
    
\end{itemize}


If we look at the constructed tree in Figure \ref{fig:example_noutlier_tree_color}, we can see that the resulting leaf is relatively deep, considering the depth of the deepest leaf.


\begin{figure}[htbp]
\centering
\includesvg[angle=90,inkscapelatex=false,width=1\textwidth]{figures/example66_Noutlier_tree.svg}
\caption{Tree constructed using the original approach}
\label{fig:example_noutlier_tree_color}
\end{figure}



\paragraph{Solution: Novelty approach}

The novelty approach, on the other hand, is a semi-supervised method. We feed the whole input (excluding the $P_x$) to the forest and examine the resulting tree.

First, the trivial binary tree $T_0$ with initial ranges is created (xx).
By doing the recursive steps, the whole tree is constructed.
Figure \ref{fig:example_novelty_tree_color} shows the constructed binary tree based on the input data.

The evaluation of $P_x$ is as follows:
\begin{itemize}
    \item In the first step, since the root node has dimension $d=1$ and split point $z = 54$ assigned during the training phase, the point visits the node $\langle 3.0, 54.0\rangle \times \langle 3.0, 105.0\rangle$
    \item In the second step, the point visits the node $\langle 3.0, 28.5\rangle \times \langle 3.0, 105.0\rangle$.
    \item This continues until the final leaf (marked grey in Figure \ref{fig:example_novelty_tree_color}) is reached. This leaf has a depth of 3.
    
\end{itemize}


Now, if we look at the resulting tree in Figure \ref{fig:example_novelty_tree_color}, we can see that the resulting leaf is relatively close to the root considering the rest of the leaves.


\begin{figure}[htbp]
\centering
\includesvg[angle=90,inkscapelatex=false,width=1\textwidth]{figures/example66_Novelty_tree.svg}
\caption{Tree constructed using the enhanced novelty approach}
\label{fig:example_novelty_tree_color}
\end{figure}
