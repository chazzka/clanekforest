\section{Proposed Half-Space Tree novelty detector}
\label{sec:novelty_isolation_forest}
This section proposes a new novelty detector based on the Half-Space tree algorithm.
This modification takes the basic idea of an ensemble of trees with various depths but takes it further to make semi-supervised detection possible.

\subsection{The Solution}
The proposed solution comes from the idea that the original tree lacks the possibility to isolate more data points than it currently observes.
The observed space is bounded by the minimum and maximum in each dimension.

As in the original article, we use the concept of a binary decision tree. The proposed solution is altering the idea of the split point evaluation. Whereas the original Isolation Forest evaluates the split point based on the previous data, the HST algorithm evaluates the split point based on a range. For this to work, several alterations to the split point evaluation and the form of data passed between vertices must be made; however, the overall concept of the forest remains the same.
The HST algorithm has several key concepts, which we adopted and modified as follows:

\begin{enumerate}
    \item We start with the root, representing the whole domain space bounded by ranges.
    \item Descendants cover the whole observable space of their associated ancestors. 
    \item The split point is in the \emph{middle} of the given dimensionâ€™s range.
    \item The input data is only used to determine the ending condition.
\end{enumerate}

\subsection{Constructing the decision tree $T$}

\begin{enumerate}
    % \item The maximum possible depth of a tree is controlled by the \emph{max\_depth} parameter.
    \item The sample \(S\) is the nonempty set of input points.
    \item Leaves and internal vertices are possibility-space hyperrectangles. 
    \item A hyperrectangle $R$ satisfies the ending condition when \(S \cap R = \emptyset\) or \(| S \cap R | = 1\).
\end{enumerate}



\paragraph{Basis step}
Each dimension \(d \in\{0, \dots, n-1\}\), is bounded by the range \(r_d\). The ranges form the possibility-space hyperrectangle \(R_0\), i.e.
\[R_0 =  r_0 \times r_1 \times \cdots \times r_{n-1}.\]

The trivial rooted tree \(T_0\) is a tuple with
vertices \(V_0 = \{R_0\}\) and edges \(E_0 = \emptyset\), i.e.
\[T_0= (V_0, E_0) = (\{R_0\},\emptyset).\]

\paragraph{Recursive step}
The steps to reach the tree \(T_{j+1}\) from \(T_{j} = (V_j, E_j)\) are
as follows:

Let \(L_j \subseteq V_j\) be a subset of leaves not satisfying the
ending condition.
For each leaf \(R \in L_j\) we
select a random dimension \(d\).

Let
\begin{align*}
R &= r_0 \times  \cdots \times r_{d-1} \times  r_d\times r_{d+1} \times \cdots \times r_{n-1},
\end{align*}
where $r_d = \langle x, y )$.
Then, we obtain the left and right hyperrectangles
\begin{align*}
R_l &= r_1 \times  \cdots \times r_{d-1} \times  \langle x, s ) \times r_{d+1} \times \cdots \times r_n, \\
R_r &= r_1 \times  \cdots \times r_{d-1} \times \langle s, y ) \times r_{d+1} \times \cdots \times r_n,
\end{align*}
where \(s = \frac{x + y}{2}\) is a number obtained as the middle of the range \(r_d\,\).

In the new tree $T_{j+1}$ each vertex \(R\) is associated with two new
edges \((R,R_l ), (R, R_r)\) giving
\begin{align*}
V_{j+1} &= V_j \cup \bigcup_{R \in L_j} \{R_l, R_r\},\\
E_{j+1} &= E_j \cup \bigcup_{R \in L_j} \{(R, R_l), (R,R_r)\},\\
T_{j+1} &= (V_{j+1}, E_{j+1}),
\end{align*}
i.e.~${R_l, R_r} \subset R$ are leaves in the new tree
\(T_{j+1}\).

\paragraph{Termination} Recursion is terminated if there is an equality of two consecutive trees \(T_k = T_{k+1}\). This happens when all leaves satisfy the ending condition, i.e., \(L_k = \emptyset\).
If this is the case, the desired tree $T$ is the tree $T_k$; otherwise, move to the next recursion step.


Note that the tree \(T_{j}\) is actually a Hasse diagram of the ordered set
\((V_j,\subseteq)\).


\begin{example}
\label{example:novelty_tree_create}
Consider now an example of creating a new HST based on the given input sample
\begin{align*}
    S = \{&[25,100],[30,90],[20,90],[35,85],\\
    &[25,85],[15,85],[105,20],[95,25], \\
    &[95,15],[90,30],[90,20],[90,10]\}.
\end{align*}
Figure \ref{fig:example_novelty_gnu} shows $S$ in the finished plane created using an enhanced approach. Observe that the split points (red lines) are always in the middle of the previous observable space, and rectangles now always cover the whole ancestor's area. Numbers represent the final leaves' depth (also seen in Figure \ref{fig:example_novelty_tree_color}).

\begin{figure*}[!t]
\centering
\includesvg[width=0.9\textwidth,inkscapelatex=false]{figures/example6_novelty_gnu_bigfont.svg}
\caption{Enhanced approach. Rectangles created by recursive splitting.}
\label{fig:example_novelty_gnu}
\end{figure*}

The tree $T$ is created by starting with the tree $T_0$ and expanding further as described by the recursive step until the ending condition is met.
Figure \ref{fig:example_novelty_tree_color} shows the finished tree $T$, trained on the dataset $S$.

\paragraph{Basis step} 
The trivial step is to create the tree $T_0$ with one root vertex $R$ with the experimentally set initial possibility space  $R= \langle 0,110) \times \langle -5,105)$ and no edges $E_0$, such that
    \begin{align*}
        V_0 &= \{R\},&
        E_0 &= \emptyset,&
        T_0 &= (V_0, E_0).
    \end{align*}

\paragraph{First recursive step}
     Since $R \in L_0$, we create two rectangles $R_l$, $R_r$ by selecting a random dimension $d=0$.
    \begin{align*}
        r_0 &= \langle 0, 110), &
        s &= \frac{0 + 110}{2} = 55, \\
        R_l &= \langle 0, 55) \times \langle -5,105), &
        R_r &= \langle 55, 110) \times \langle -5,105).
    \end{align*}
     New tree $T_1$ is then
    \begin{align*}
    V_1 &= \{R, R_l, R_r\}, &
    E_1 &= \{(R, R_l), (R, R_r)\}, &
    T_1 &= (V_1, E_1).
    \end{align*}
    Checking for the ending condition, set $L_1$ contains precisely two elements $R_l$ and $R_r$; hence, we continue.
     
\paragraph{Second recursive step}
    Since there are vertices in $L_1$ left to be examined, we continue with recursive steps.
    Since $R_l \in L_1$ (resp. $R_r \in L_1$), we create two new rectangles $R_{ll}$, $R_{lr}$ -- left column (resp. $R_{rl}$, $R_{rr}$ -- right column) by selecting a random dimension $d=1$ (resp. $d=0$).
    \begin{align*}
        r_{l1} &= \langle -5, 105)& r_{r0} &= \langle 55, 110) \\
        s_l &= 50 & s_r&=82.5\\
        R_{ll} &= \langle 0, 55) \times \langle -5,50) & R_{rl} &= \langle 55, 82.5) \times \langle -5,105)\\
        R_{lr} &= \langle 0, 55) \times \langle 50,105) & R_{rr} &= \langle 82.5, 110) \times \langle -5,105)
    \end{align*}
 New tree $T_2$ is then
    \begin{align*}
        V_2 &= \{R, R_l, R_r, R_{ll}, R_{lr}, R_{rl}, R_{rr}\} \\
        E_2 &= \{(R, R_l), (R, R_r), (R_l, R_{ll}), (R_l, R_{lr}), (R_r, R_{rl}), (R_r, R_{rr})\} \\
        T_2 &= (V_2, E_2)
    \end{align*}
\paragraph{Termination} We recheck the ending condition. The set of leaves $L_2$ now contains two hyperrectangles $R_{lr}$ and $R_{rr}$.
This goes on until the ending condition is met. The resulting tree $T_8$ is depicted in Figure \ref{fig:example_novelty_tree_color}.
\end{example}

\subsection{Evaluating the decision tree $T$}
The evaluation of this decision tree is more straightforward since the examined data point is always contained in the possibility space hyperrectangle of some vertex in each level of depth until a leaf is visited.
The evaluation starts in the root vertex. The initial possibility space should be reasonable enough to cover the whole domain of a given problem. Until the leaf is reached, the examined point recursively visits the descendant within which it is contained.



\begin{example}
\label{ex:regular_point_evaluation_novelty}
    Consider now the evaluation of $a = [105,20]$ on the tree $T$ built in Example \ref{example:novelty_tree_create}.

\begin{enumerate}
    \item  We start with the root $R = \langle 0,110\rangle \times \langle -5, 105 \rangle$.
    The root $R$ has two descendants 
\begin{align*}
    &R_l = \langle 0,55) \times \langle -5, 105),&
    &R_r = \langle 55,110) \times \langle -5, 105),
\end{align*}
Since $a \in R_r$, we visit $R_r$.
\item Vertex $R_r$ has two descendants
\begin{align*}
    &R_{rl} = \langle 5,82.5) \times \langle -5, 105),&
    &R_{rr} = \langle 82.5,110) \times \langle -5, 105),
\end{align*}
Since $a \in R_{rr}$, we visit the vertex $R_{rr}$.
\item
We continue recursively for another two steps until the leaf $R_{rrlr}$ is reached. This leaf has a depth of $4$.

\end{enumerate}
   
\end{example}

\begin{example}
\label{ex:novelty_point_evaluation_novelty}
    Consider now the evaluation of $a' = [25,20]$ on the tree $T$ built in Example \ref{example:novelty_tree_create}.

\begin{enumerate}
    \item  We start with the root $R = \langle 0,110\rangle \times \langle -5, 105 \rangle$.
    The root $R$ has two descendants 
\begin{align*}
    &R_l = \langle 0,55) \times \langle -5, 105),&
    &R_r = \langle 55,110) \times \langle -5, 105),
\end{align*}
Since $a' \in R_l$, we visit $R_l$.
\item Vertex $R_l$ has two descendants
\begin{align*}
    &R_{ll} = \langle 0,55) \times \langle -5, 50),&
    &R_{lr} = \langle 0,5) \times \langle 50, 105),
\end{align*}
Since $a' \in R_{ll}$, we visit the vertex $R_{ll}$.
\item
Since $R_{ll}$ is a leaf, we end here, and the reached leaf's depth is $2$. The leaf is marked gray in Figure \ref{fig:example_novelty_tree_color}.
\end{enumerate}
\end{example}
Note that each evaluated point of the given possibility space is always contained in each vertex it visits.
Hence $a \in R_{rrlr} \subset R_{rrl} \subset R_{rr} \subset R_{r} \subset R$ in Example \ref{ex:regular_point_evaluation_novelty}
and $a' \in R_{ll} \subset R_{l} \subset R$ as shown in Example \ref{ex:novelty_point_evaluation_novelty}.


%z tadyka jsem smazal puvodni evaluaci, je v evaluate_old.tex



\begin{sidewaysfigure}[htbp]
\centering
\includesvg[inkscapelatex=false,width=1\textwidth]{figures/example6_Novelty_tree_tb.svg}
\caption{Tree constructed using the enhanced novelty approach.}
\label{fig:example_novelty_tree_color}
\end{sidewaysfigure}


