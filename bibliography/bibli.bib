@inproceedings{liu2008isolation,
  title={Isolation forest},
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  booktitle={2008 eighth ieee international conference on data mining},
  pages={413--422},
  year={2008},
  organization={IEEE}
}

@article{liu2012isolation,
  title={Isolation-based anomaly detection},
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={6},
  number={1},
  pages={1--39},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@book{rosen2012discrete,
  title={Discrete Mathematics and Its Applications: With Combinatorics and Graph Theory},
  author={Rosen, K.H. and Krithivasan, K.},
  isbn={9780070681880},
  lccn={2011011060},
  url={https://books.google.cz/books?id=mcxHzwEACAAJ},
  year={2012},
  publisher={McGraw-Hill Companies}
}

@inproceedings{agrawal1994fast,
  title={Fast algorithms for mining association rules},
  author={Agrawal, Rakesh and Srikant, Ramakrishnan and others},
  booktitle={Proc. 20th int. conf. very large data bases, VLDB},
  volume={1215},
  pages={487--499},
  year={1994},
  organization={Santiago}
}

@inproceedings{agrawal1995mining,
  title={Mining sequential patterns},
  author={Agrawal, Rakesh and Srikant, Ramakrishnan},
  booktitle={Proceedings of the eleventh international conference on data engineering},
  pages={3--14},
  year={1995},
  organization={IEEE}
}

@inproceedings{Ester1996dbscan,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
year = {1996},
publisher = {AAAI Press},
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
pages = {226–231},
numpages = {6},
keywords = {handling nlj4-275oise, efficiency on large spatial databases, clustering algorithms, arbitrary shape of clusters},
location = {Portland, Oregon},
series = {KDD'96}
}

@ARTICLE{lloyd1982kmeans,
  author={Lloyd, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={Least squares quantization in PCM}, 
  year={1982},
  volume={28},
  number={2},
  pages={129-137},
  keywords={},
  doi={10.1109/TIT.1982.1056489}}

@book{grubbs1949sample,
  title={Sample criteria for testing outlying observations},
  author={Grubbs, Frank Ephraim},
  year={1949},
  publisher={University of Michigan}
}

@article{he2003discovering,
  title={Discovering cluster-based local outliers},
  author={He, Zengyou and Xu, Xiaofei and Deng, Shengchun},
  journal={Pattern recognition letters},
  volume={24},
  number={9-10},
  pages={1641--1650},
  year={2003},
  publisher={Elsevier}
}

@article{MARKOU20032481,
title = {Novelty detection: a review—part 1: statistical approaches},
journal = {Signal Processing},
volume = {83},
number = {12},
pages = {2481-2497},
year = {2003},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2003.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0165168403002020},
author = {Markos Markou and Sameer Singh},
keywords = {Novelty detection review, Statistical approaches, Gaussian mixture models, Hidden Markov models, KNN, Parzen density estimation, String matching, Clustering},
abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. Novelty detection is one of the fundamental requirements of a good classification or identification system since sometimes the test data contains information about objects that were not known at the time of training the model. In this paper we provide state-of-the-art review in the area of novelty detection based on statistical approaches. The second part paper details novelty detection using neural networks. As discussed, there are a multitude of applications where novelty detection is extremely important including signal processing, computer vision, pattern recognition, data mining, and robotics.}
}

@article{tax2004support,
  title={Support vector data description},
  author={Tax, David MJ and Duin, Robert PW},
  journal={Machine learning},
  volume={54},
  pages={45--66},
  year={2004},
  publisher={Springer}
}

@article{ZHOU20022927,
title = {Linear programming support vector machines},
journal = {Pattern Recognition},
volume = {35},
number = {12},
pages = {2927-2936},
year = {2002},
note = {Pattern Recognition in Information Systems},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(01)00210-2},
url = {https://www.sciencedirect.com/science/article/pii/S0031320301002102},
author = {Weida Zhou and Li Zhang and Licheng Jiao},
keywords = {Statistical learning theory, VC dimension, Support vector machines, Generalization performance, Linear programming},
abstract = {Based on the analysis of the conclusions in the statistical learning theory, especially the VC dimension of linear functions, linear programming support vector machines (or SVMs) are presented including linear programming linear and nonlinear SVMs. In linear programming SVMs, in order to improve the speed of the training time, the bound of the VC dimension is loosened properly. Simulation results for both artificial and real data show that the generalization performance of our method is a good approximation of SVMs and the computation complex is largely reduced by our method.}
}

@inproceedings{breunig2000lof,
  title={LOF: identifying density-based local outliers},
  author={Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  booktitle={Proceedings of the 2000 ACM SIGMOD international conference on Management of data},
  pages={93--104},
  year={2000}
}